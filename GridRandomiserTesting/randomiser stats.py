""" Analyse randomly generated lists of consecutive integers corresponding to n x n grids """

import pandas as pd
import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
from math import factorial as fct


def analyse_dataset(dataset_file, grid_size):
    # open the file containing the data and create array with results
    unique_grids = dict()
    with open(dataset_file, 'r') as grid_dataset:
        # Use a dictionary to accumulate unique grids and their frequencies to be used for data analysis. Each grid,
        # which is a in the dataset (txt file), is used as a key for the dictionary. If there is no value associated
        # with it yet, then the value is set to 1. If there is, then increment the value by 1.
        # Thus, every unique grid from the dataset is a key in the dictionary, and the associated value is the
        # frequency of that grid being generated.
        counter = 0
        for line in grid_dataset:
            # if no key in dict for that grid, add it and set value to 1 for the frequency
            if unique_grids.get(line) is None:
                unique_grids[line] = 1
            else:  # if key is already present, increase value by 1
                unique_grids[line] += 1
            counter += 1
    grid_dataset.close()  # think with closes the file but kept this anyway...

    # make a dataframe (used to display data in plots) with two columns :
    # "grid", gives a unique grid as a string of the numbers that make up the grid
    # "frequency", gives the frequency of the unique grid in the dictionary (how many were generated)
    df = pd.DataFrame({"grid": [grid for grid in unique_grids],
                       "frequency": [unique_grids[key] for key in unique_grids]})

    # print out some stats regarding the dataset: number of possible unique (solvable) grids, number of unique grids
    # generated, mean/median/std dev/min/max for frequency of unique grids
    num_solvable_combinations = int(fct(grid_size)/2)  # factorials always even...use int
    num_unique_grids = len(unique_grids)
    percent_grids_created = num_unique_grids/num_solvable_combinations*100
    percent_unique = num_unique_grids/counter
    print("\nThere are {:d} possible (solvable) combinations \nof {:d} consecutive"
          " integers in a grid.".format(num_solvable_combinations, grid_size))
    print("{:.3f}% ({:d}) of possible unique grids were \nfound from a total of {:d}" 
          " generated.\n".format(percent_grids_created, num_unique_grids, counter))
    print(df.describe())

    # Sorted unique grid frequency values in ascending order and displayed the data in a scatter plot and histogram.
    # These clearly display how many of each unique grid was generated by the randomiser, which can show whether or not
    # the grids are generated in an appropriately random manner. If every possible unique grid is created, with
    # relatively small spread of frequencies (not favouring any particular grid), then it is sufficiently random.
    # TODO: try multiple datasets of same size, track one grid such as the lowest/highest frequency in 1st dataset
    # TODO: this one grid should have a different of frequency each time
    freq_sorted_series = df['frequency'].sort_values()
    # plt.scatter(df.index, freq_sorted_series, 1)
    plt.scatter(df.index, df['frequency'], 0.5)
    plt.title("Scatter plot for frequency of unique lists, in a dataset of\n "
              + str(counter) + " randomly generated lists of values 1 through " + str(grid_size))
    plt.xlabel("Index of list in dataset")
    plt.ylabel("Frequency of unique lists")
    plt.show()
    plt.hist(freq_sorted_series, bins=15)
    plt.title("Histogram to display the spread of different frequencies of\n unique lists from a dataset"
              " of " + str(counter) + " generated lists")
    plt.xlabel("Unique frequency values")
    plt.ylabel("Frequency of frequency unique lists")
    plt.grid = True
    plt.show()

    # TODO: can test how a change in function might affect randomness for instance, can swap a random cell with random
    # neighbour or any other cell then check if it has changed inversions. I think swapping wih 1 beside changes
    # inversions by set amount? think of an unsolvable puzzle in closest to solved
    # position, there will be 13, 15, 14, _ in last row, meaning 1 is out of place and only requires one swap


analyse_dataset("gridsize(15)_numgrids(1000000).txt", 15)


def use_list():
    # TODO: Tested using list, but is very slow as I thought (hence why originally used dict)
    unique_grids_array = []
    counter = 0
    with open('3grid10mil.txt', 'r') as grid_dataset:
        for grid in grid_dataset:
            try:
                grid_array_index = unique_grids_array.index(grid)
                unique_grids_array[grid_array_index][1] += 1
                print('test')
            except ValueError as e:
                print(e)
                unique_grids_array.append([grid, 1])
            counter += 1
            print(counter)
            print(unique_grids_array)
        print('test2')


# use_list()
